import torch
import torch.nn as nn
import numpy as np
import json
import os
from model import BiDAF

def word_tokenize(text):
    # Tokenizer ƒë∆°n gi·∫£n t∆∞∆°ng ·ª©ng v·ªõi qu√° tr√¨nh preprocess
    return text.lower().replace('.', ' . ').replace(',', ' , ').replace('?', ' ? ').split()

def demo():
    # --- 1. C·∫•u h√¨nh ---
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    checkpoint_path = 'save/bidaf_epoch_22.pt' # S·ª≠ d·ª•ng epoch 22 nh∆∞ ƒë√£ th·∫£o lu·∫≠n
    hidden_size = 100

    if not os.path.exists(checkpoint_path):
        print(f"‚ùå Kh√¥ng t√¨m th·∫•y file checkpoint t·∫°i {checkpoint_path}")
        return

    # --- 2. T·∫£i t·ª´ ƒëi·ªÉn ---
    print("üîÑ ƒêang n·∫°p t·ª´ ƒëi·ªÉn v√† m√¥ h√¨nh...")
    with open('data/processed/word2idx.json', 'r', encoding='utf-8') as f:
        word2idx = json.load(f)
    with open('data/processed/char2idx.json', 'r', encoding='utf-8') as f:
        char2idx = json.load(f)
    
    word_vectors = torch.from_numpy(np.load('data/processed/word_emb.npy')).float()
    char_vocab_size = len(char2idx)

    # --- 3. Kh·ªüi t·∫°o v√† n·∫°p m√¥ h√¨nh ---
    model = BiDAF(word_vectors, char_vocab_size, hidden_size).to(device)
    model.load_state_dict(torch.load(checkpoint_path, map_location=device))
    model.eval()
    print("‚úÖ ƒê√£ s·∫µn s√†ng!\n")

    while True:
        print("-" * 50)
        context = input("Nh·∫≠p ƒëo·∫°n vƒÉn (ho·∫∑c 'q' ƒë·ªÉ tho√°t): ")
        if context.lower() == 'q': break
        
        question = input("Nh·∫≠p c√¢u h·ªèi: ")
        if question.lower() == 'q': break

        # --- 4. Ti·ªÅn x·ª≠ l√Ω d·ªØ li·ªáu nh·∫≠p v√†o ---
        c_tokens = word_tokenize(context)
        q_tokens = word_tokenize(question)

        # Chuy·ªÉn th√†nh Word Index
        cw = torch.LongTensor([word2idx.get(w, word2idx.get('<UNK>', 1)) for w in c_tokens]).unsqueeze(0).to(device)
        qw = torch.LongTensor([word2idx.get(w, word2idx.get('<UNK>', 1)) for w in q_tokens]).unsqueeze(0).to(device)

        # Chuy·ªÉn th√†nh Char Index (Max word length = 16)
        def get_char_tensor(tokens):
            ct = torch.zeros(len(tokens), 16).long()
            for i, w in enumerate(tokens):
                for j, c in enumerate(w[:16]):
                    ct[i, j] = char2idx.get(c, char2idx.get('<UNK>', 1))
            return ct.unsqueeze(0).to(device)

        cc = get_char_tensor(c_tokens)
        qc = get_char_tensor(q_tokens)

        # --- 5. D·ª± ƒëo√°n ---
        with torch.no_grad():
            p1, p2 = model(cw, cc, qw, qc)
            
            # L·∫•y v·ªã tr√≠ b·∫Øt ƒë·∫ßu v√† k·∫øt th√∫c c√≥ x√°c su·∫•t cao nh·∫•t
            s_idx = torch.argmax(p1, dim=1).item()
            e_idx = torch.argmax(p2, dim=1).item()

            if s_idx <= e_idx:
                answer = " ".join(c_tokens[s_idx : e_idx + 1])
            else:
                answer = "Xin l·ªói, t√¥i kh√¥ng t√¨m th·∫•y c√¢u tr·∫£ l·ªùi ph√π h·ª£p."

        print(f"\nü§ñ Robot tr·∫£ l·ªùi: {answer.capitalize()}")

if __name__ == "__main__":
    demo()