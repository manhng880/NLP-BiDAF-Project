import json
import torch
import numpy as np
from torch.utils.data import Dataset, DataLoader

# Trong file data_loader.py

class FolkloreDataset(Dataset):
    # TƒÉng max_context_len l√™n 500 ho·∫∑c cao h∆°n t√πy ƒë·ªô d√†i truy·ªán c·ªßa b·∫°n
    def __init__(self, data_path, word2idx_path, max_context_len=600, max_question_len=50):
        with open(data_path, 'r', encoding='utf-8') as f:
            self.data = json.load(f)['data']
        with open(word2idx_path, 'r', encoding='utf-8') as f:
            self.word2idx = json.load(f)
            
        self.max_c_len = max_context_len
        self.max_q_len = max_question_len
        
        chars = "abcdeghiklmnopqrstuvxy√†√°·∫£√£·∫°√¢·∫ß·∫•·∫©·∫´·∫≠ƒÉ·∫±·∫Ø·∫≥·∫µ·∫∑√®√©·∫ª·∫Ω·∫π√™·ªÅ·∫ø·ªÉ·ªÖ·ªá√¨√≠·ªâƒ©·ªã√≤√≥·ªè√µ·ªç√¥·ªì·ªë·ªï·ªó·ªô∆°·ªù·ªõ·ªü·ª°·ª£√π√∫·ªß≈©·ª•∆∞·ª´·ª©·ª≠·ªØ·ª±·ª≥√Ω·ª∑·ªπ·ªµƒë_ "
        self.char2idx = {char: i + 2 for i, char in enumerate(chars)}
        self.char2idx["<PAD>"] = 0
        self.char2idx["<UNK>"] = 1

        self.samples = self._process_data()
        # Th√™m d√≤ng n√†y ƒë·ªÉ ki·ªÉm tra l·ªói
        print(f"‚úÖ ƒê√£ n·∫°p th√†nh c√¥ng {len(self.samples)} m·∫´u t·ª´ {data_path}")

    def _process_data(self):
        samples = []
        skipped_error = 0
        skipped_length = 0
        
        for story in self.data:
            for p in story['paragraphs']:
                # T√°ch t·ª´ context
                context_tokens = p['context'].lower().split()
                
                for qa in p['qas']:
                    # 1. Ki·ªÉm tra s·ª± t·ªìn t·∫°i c·ªßa nh√£n word index
                    if 'answer_start_word' not in qa or 'answer_end_word' not in qa:
                        skipped_error += 1
                        continue
                        
                    ans_start = qa['answer_start_word']
                    ans_end = qa['answer_end_word']
                    
                    # 2. Lo·∫°i b·ªè nh√£n -1 (L·ªói kh√¥ng t√¨m th·∫•y t·ª´)
                    if ans_start == -1 or ans_end == -1:
                        skipped_error += 1
                        continue
                    
                    # 3. Lo·∫°i b·ªè n·∫øu nh√£n v∆∞·ª£t qu√° ƒë·ªô d√†i t·ªëi ƒëa (max_c_len)
                    if ans_start >= self.max_c_len or ans_end >= self.max_c_len:
                        skipped_length += 1
                        continue
                    
                    # 4. ƒê·∫£m b·∫£o start kh√¥ng l·ªõn h∆°n end
                    if ans_start > ans_end:
                        skipped_error += 1
                        continue

                    # X·ª≠ l√Ω c√¢u h·ªèi
                    question_tokens = qa['question'].lower().split()[:self.max_q_len]
                    
                    # Chuy·ªÉn th√†nh ID
                    c_idxs = [self.word2idx.get(w, 1) for w in context_tokens[:self.max_c_len]]
                    q_idxs = [self.word2idx.get(w, 1) for w in question_tokens]
                    
                    samples.append({
                        'c_word': c_idxs,
                        'q_word': q_idxs,
                        's_idx': ans_start,
                        'e_idx': ans_end,
                        'context_raw': context_tokens[:self.max_c_len]
                    })
        
        print(f"üìä Th·ªëng k√™: Gi·ªØ l·∫°i {len(samples)} m·∫´u. B·ªè qua {skipped_error} m·∫´u l·ªói nh√£n v√† {skipped_length} m·∫´u qu√° d√†i.")
        return samples

    def __len__(self):
        return len(self.samples)

    def __getitem__(self, idx):
        s = self.samples[idx]
        
        # Padding word
        c_word = np.zeros(self.max_c_len, dtype=np.int64)
        c_word[:len(s['c_word'])] = s['c_word']
        
        q_word = np.zeros(self.max_q_len, dtype=np.int64)
        q_word[:len(s['q_word'])] = s['q_word']
        
        return {
            "c_word": torch.tensor(c_word),
            "q_word": torch.tensor(q_word),
            "y1": torch.tensor(s['s_idx']),
            "y2": torch.tensor(s['e_idx'])
        }

def get_loader(data_path, word2idx_path, batch_size=4):
    dataset = FolkloreDataset(data_path, word2idx_path)
    return DataLoader(dataset, batch_size=batch_size, shuffle=True)