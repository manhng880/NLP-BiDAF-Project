import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import os
import re
from tqdm import tqdm
from data_loader import get_loader
from model import BiDAF

def get_latest_checkpoint(save_dir):
    """TÃ¬m file epoch cao nháº¥t trong thÆ° má»¥c save"""
    if not os.path.exists(save_dir):
        return None, 0
    
    files = [f for f in os.listdir(save_dir) if f.endswith('.pt') and 'epoch_' in f]
    if not files:
        return None, 0
    
    # TrÃ­ch xuáº¥t sá»‘ epoch tá»« tÃªn file (vd: bidaf_epoch_10.pt -> 10)
    epochs = [int(re.findall(r'\d+', f)[0]) for f in files]
    max_epoch = max(epochs)
    latest_file = os.path.join(save_dir, f'bidaf_epoch_{max_epoch}.pt')
    
    return latest_file, max_epoch

def train():
    # --- 1. Cáº¥u hÃ¬nh ---
    device = torch.device('cpu')
    batch_size = 4
    hidden_size = 100
    step_size = 10    # Sá»‘ lÆ°á»£ng epoch cháº¡y má»—i láº§n
    lr = 0.5  
    save_dir = 'save'
    
    if not os.path.exists(save_dir):
        os.makedirs(save_dir)

    # --- 2. Táº£i dá»¯ liá»‡u ---
    print("ğŸš€ Äang táº£i dá»¯ liá»‡u vÃ  Embedding...")
    word_vectors = torch.from_numpy(np.load('data/processed/word_emb.npy')).float()
    train_loader = get_loader('data/train.json', 'data/processed/word2idx.json', batch_size=batch_size)
    
    # --- 3. Khá»Ÿi táº¡o mÃ´ hÃ¬nh ---
    model = BiDAF(word_vectors=word_vectors, char_vocab_size=100, emb_dim=300, hidden_size=hidden_size).to(device)
    optimizer = optim.Adadelta(model.parameters(), lr=lr)
    criterion = nn.CrossEntropyLoss()

    # --- 4. Tá»± Ä‘á»™ng tÃ¬m checkpoint má»›i nháº¥t ---
    latest_file, start_epoch = get_latest_checkpoint(save_dir)
    
    # TÃ­nh toÃ¡n Ä‘iá»ƒm dá»«ng cho láº§n cháº¡y nÃ y
    target_epoch = start_epoch + step_size
    
    if latest_file:
        print(f"ğŸ”„ TÃ¬m tháº¥y checkpoint má»›i nháº¥t: {latest_file}")
        print(f"ğŸ“¥ Náº¡p trá»ng sá»‘ tá»« Epoch {start_epoch}...")
        model.load_state_dict(torch.load(latest_file, map_location=device))
        print(f"â–¶ï¸ Cháº¡y tiáº¿p {step_size} epoch (tá»« {start_epoch + 1} Ä‘áº¿n {target_epoch})")
    else:
        print(f"ğŸ†• Huáº¥n luyá»‡n tá»« Ä‘áº§u. Cháº¡y {step_size} epoch (Ä‘áº¿n Epoch {target_epoch})")
        start_epoch = 0

    # --- 5. VÃ²ng láº·p huáº¥n luyá»‡n ---
    for epoch in range(start_epoch, target_epoch):
        model.train()
        epoch_loss = 0
        
        progress_bar = tqdm(train_loader, desc=f"Epoch {epoch+1}/{target_epoch}")
        for batch in progress_bar:
            optimizer.zero_grad()
            
            cw = batch['c_word'].to(device)
            qw = batch['q_word'].to(device)
            y1 = batch['y1'].to(device)
            y2 = batch['y2'].to(device)

            p1, p2 = model(cw, qw)
            loss = criterion(p1, y1) + criterion(p2, y2)
            
            loss.backward()
            optimizer.step()
            
            epoch_loss += loss.item()
            progress_bar.set_postfix(loss=loss.item())

        avg_loss = epoch_loss / len(train_loader)
        print(f"âœ¨ Epoch {epoch+1} hoÃ n táº¥t. Loss trung bÃ¬nh: {avg_loss:.4f}")
        
        # LÆ°u file cho tá»«ng epoch
        save_path = os.path.join(save_dir, f'bidaf_epoch_{epoch+1}.pt')
        torch.save(model.state_dict(), save_path)
        print(f"ğŸ’¾ ÄÃ£ lÆ°u: {save_path}")

    print(f"\nâœ… ÄÃ£ hoÃ n thÃ nh Ä‘á»£t huáº¥n luyá»‡n nÃ y ({step_size} epoch).")
    print(f"ğŸ“ Tá»•ng cá»™ng Ä‘Ã£ huáº¥n luyá»‡n Ä‘Æ°á»£c: {target_epoch} epoch.")

if __name__ == "__main__":
    train()