import torch
import numpy as np
import json
import os
from model import BiDAF

def predict(context_text, question_text, model_path, word2idx_path):
    device = torch.device('cpu')
    
    # 1. Load tá»« Ä‘iá»ƒn
    with open(word2idx_path, 'r', encoding='utf-8') as f:
        word2idx = json.load(f)
    
    # 2. Load Model
    word_vectors = torch.from_numpy(np.load('data/processed/word_emb.npy')).float()
    model = BiDAF(word_vectors=word_vectors, char_vocab_size=100, emb_dim=300, hidden_size=100)
    
    if not os.path.exists(model_path):
        return "Lá»—i: KhÃ´ng tÃ¬m tháº¥y file model Ä‘Ã£ huáº¥n luyá»‡n!"
        
    model.load_state_dict(torch.load(model_path, map_location=device))
    model.eval()

    # 3. Tiá»n xá»­ lÃ½
    c_tokens = context_text.lower().split()
    q_tokens = question_text.lower().split()
    
    # Padding/Truncate context vá» Ä‘Ãºng max_len (600 nhÆ° trong data_loader)
    c_idx_list = [word2idx.get(w, 1) for w in c_tokens[:600]]
    if len(c_idx_list) < 600:
        c_idx_list += [0] * (600 - len(c_idx_list))
        
    q_idx_list = [word2idx.get(w, 1) for w in q_tokens[:50]]
    if len(q_idx_list) < 50:
        q_idx_list += [0] * (50 - len(q_idx_list))

    c_idx = torch.tensor(c_idx_list).unsqueeze(0)
    q_idx = torch.tensor(q_idx_list).unsqueeze(0)

    # 4. Dá»± Ä‘oÃ¡n
    with torch.no_grad():
        p1, p2 = model(c_idx, q_idx) # Output: [1, 600]
        
        # Ãp dá»¥ng Softmax Ä‘á»ƒ láº¥y xÃ¡c suáº¥t
        p1 = torch.softmax(p1, dim=1)
        p2 = torch.softmax(p2, dim=1)
        
        # TÃ¬m cáº·p (i, j) sao cho i <= j vÃ  p1[i]*p2[j] lá»›n nháº¥t
        max_prob = 0
        start_idx = 0
        end_idx = 0
        
        # Thuáº­t toÃ¡n tÃ¬m kiáº¿m tá»‘i Æ°u cho QA
        for i in range(len(c_tokens[:600])):
            for j in range(i, min(i + 15, len(c_tokens[:600]))): # Giáº£ sá»­ cÃ¢u tráº£ lá»i khÃ´ng quÃ¡ 15 tá»«
                prob = p1[0, i] * p2[0, j]
                if prob > max_prob:
                    max_prob = prob
                    start_idx = i
                    end_idx = j

    # 5. TrÃ­ch xuáº¥t
    prediction = " ".join(c_tokens[start_idx : end_idx + 1])
    return prediction, start_idx, end_idx

if __name__ == "__main__":
    # Tá»± Ä‘á»™ng láº¥y dá»¯ liá»‡u tá»« dev.json Ä‘á»ƒ test
    with open('data/dev.json', 'r', encoding='utf-8') as f:
        dev_data = json.load(f)['data']

    # Láº¥y máº«u Ä‘áº§u tiÃªn
    story = dev_data[2]['paragraphs'][0]
    context = story['context']
    qa = story['qas'][0]
    question = qa['question']
    target = qa['answers'][0]['text']

    print(f"â“ CÃ¢u há»i: {question}")
    print(f"ğŸ¯ ÄÃ¡p Ã¡n Ä‘Ãºng: {target}")

    # Sá»­ dá»¥ng file model tá»‘t nháº¥t
    ans, s, e = predict(context, question, 'save/bidaf_best.pt', 'data/processed/word2idx.json')

    print(f"ğŸ¤– BiDAF dá»± Ä‘oÃ¡n (tá»« {s} Ä‘áº¿n {e}): {ans}")