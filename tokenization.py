import json
import os
from underthesea import word_tokenize

# C·∫•u h√¨nh file
INPUT_FILE = "vietnamese_folktale_labeled.json" # File g·ªëc b·∫°n ƒëang l√†m
OUTPUT_FILE = "vietnamese_folktale_50_stories_final.json"

def align_and_tokenize(text, original_start, answer_text):
    # 1. T√°ch t·ª´ cho to√†n b·ªô context
    # format="text" s·∫Ω d√πng d·∫•u g·∫°ch d∆∞·ªõi cho t·ª´ gh√©p v√† t√°ch d·∫•u c√¢u
    tokenized_text = word_tokenize(text, format="text")
    
    # 2. T√¨m v·ªã tr√≠ m·ªõi c·ªßa c√¢u tr·∫£ l·ªùi
    # V√¨ b·∫°n ƒë√£ label r·∫•t chu·∫©n, ch√∫ng ta s·∫Ω t√¨m b·∫£n 'ƒë√£ t√°ch t·ª´' c·ªßa c√¢u tr·∫£ l·ªùi trong context m·ªõi
    tokenized_answer = word_tokenize(answer_text, format="text")
    
    new_start = tokenized_text.find(tokenized_answer)
    
    # N·∫øu kh√¥ng t√¨m th·∫•y tr·ª±c ti·∫øp (do underthesea t√°ch kh√°c nhau), ta d√πng thu·∫≠t to√°n b√π tr·ª´
    if new_start == -1:
        # Th·ª≠ t√¨m b·∫£n kh√¥ng d·∫•u g·∫°ch d∆∞·ªõi
        clean_ans = tokenized_answer.replace("_", " ")
        clean_context = tokenized_text.replace("_", " ")
        new_start = clean_context.find(clean_ans)
        
    return tokenized_text, new_start, tokenized_answer

def process_stage_1():
    if not os.path.exists(INPUT_FILE):
        print("‚ùå Kh√¥ng t√¨m th·∫•y file g·ªëc!")
        return

    with open(INPUT_FILE, 'r', encoding='utf-8') as f:
        data = json.load(f)

    # Ch·ªâ l·∫•y 50 truy·ªán ƒë·∫ßu ti√™n
    subset_data = data['data'][:50]
    new_data = {"version": "v1.0-50stories", "data": []}
    
    count_success = 0
    count_fail = 0

    for story in subset_data:
        new_story = {"title": story.get('title', 'Kh√¥ng r√µ ti√™u ƒë·ªÅ'), "paragraphs": []}
        
        for p in story['paragraphs']:
            if not p['qas']: continue # B·ªè qua ƒëo·∫°n kh√¥ng c√≥ c√¢u h·ªèi
            
            # T√°ch t·ª´ context
            tokenized_context = word_tokenize(p['context'], format="text")
            new_p = {"context": tokenized_context, "qas": []}
            
            for qa in p['qas']:
                new_qa = {"id": qa['id'], "question": word_tokenize(qa['question'], format="text"), "answers": []}
                
                for ans in qa['answers']:
                    # CƒÉn ch·ªânh l·∫°i index
                    _, new_start, new_ans_text = align_and_tokenize(p['context'], ans['answer_start'], ans['text'])
                    
                    if new_start != -1:
                        new_qa['answers'].append({
                            "answer_start": new_start,
                            "text": new_ans_text
                        })
                        count_success += 1
                    else:
                        count_fail += 1
                
                if new_qa['answers']:
                    new_p['qas'].append(new_qa)
            
            new_story['paragraphs'].append(new_p)
        new_data['data'].append(new_story)

    with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:
        json.dump(new_data, f, ensure_ascii=False, indent=4)

    print(f"\n‚úÖ Ho√†n th√†nh 50 truy·ªán!")
    print(f"üìä Th√†nh c√¥ng: {count_success} c√¢u h·ªèi")
    print(f"‚ö†Ô∏è Th·∫•t b·∫°i (l·ªách index): {count_fail} c√¢u")

if __name__ == "__main__":
    process_stage_1()